{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4a34777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import csv\n",
    "import io\n",
    "import argparse\n",
    "\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d66b5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read a CSV file in chunks, map the primary ID to a mapping file, and keep the rows that can be mapped.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the CSV file.\n",
    "    - mapping_file: Path to the mapping file (CSV).\n",
    "    - primary_id_column: Index of the column containing the primary ID in the mapping file.\n",
    "    - out_path: Path to the output file.\n",
    "    - chunk_size: Size of each chunk. Defaults to 100,000 lines.\n",
    "    \"\"\"\n",
    "def map_goa_to_cafa_ids(file_path, mapping_file, primary_id_column, out_path, chunk_size=100000):\n",
    "    # Read the mapping file into a DataFrame\n",
    "    mapping_df = pd.read_csv(mapping_file, sep = \",\", header = 0)\n",
    "    mapping_df.columns = [\"DB Object ID\", \"CAFA4_ID\"]\n",
    "\n",
    "    # Extract the primary IDs from the mapping file and convert to a set for efficient lookup\n",
    "    id_set = set(mapping_df[\"DB Object ID\"])\n",
    "\n",
    "    # Initialize an empty list to store filtered chunk dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Read the CSV file in chunks\n",
    "    #flag = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size, sep = \"\\t\"):\n",
    "        # Filter the chunk based on whether the primary ID can be found in the mapping file\n",
    "        filtered_chunk = chunk[chunk.iloc[:,primary_id_column].isin(id_set)]\n",
    "        filtered_chunk = filtered_chunk.drop_duplicates().copy()\n",
    "        dfs.append(filtered_chunk)\n",
    "\n",
    "    # Concatenate all the filtered chunk dataframes into a single dataframe\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    df_mapped = pd.merge(df, mapping_df, on='DB Object ID', how='inner')\n",
    "    print(\"Rows in the mapped file : \", len(df_mapped))\n",
    "    \n",
    "    df_mapped = df_mapped.loc[:, [\"CAFA4_ID\", \"GO ID\", \"Aspect\"]].copy()\n",
    "\n",
    "    # Write the final dataframe to the output file\n",
    "    df_mapped.to_csv(out_path, index=False, sep = \"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7c6ef61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in the mapped file :  449581\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import multiprocessing\n",
    "\n",
    "def get_preprocess_cmd(gaf_path, out_path):\n",
    "    cmd = [\n",
    "    \"python3\",                 # Command to execute Python 3\n",
    "    \"preprocess_gaf.py\",       # Script to run\n",
    "    t0_gaf_file,  # Path to input file\n",
    "    \"--highTP\",\n",
    "    \"--out_path\", out_path,        # Output path parameter\n",
    "    #\"--evidence_codes\", \"EXP\", \"IDA\",   # Evidence codes parameter\n",
    "    #\"--extract_col_list\", \"DB Object ID\", \"Qualifier\"  # Extract column list parameter\n",
    "]\n",
    "    return cmd\n",
    "\n",
    "def run_process(command):\n",
    "    subprocess.run(command)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define commands and log file names\n",
    "    work_dir = \"/data/rashika/CAFA4/\"\n",
    "    \n",
    "    #t0_gaf_file = work_dir + \"uniprot/raw_goa/sample_t0.gz\"\n",
    "    t0_gaf_file = work_dir + \"uniprot/raw_goa/t0/goa_uniprot_all.gaf.195.gz\"\n",
    "    t0_processed = work_dir + \"extracted_goa/t0_preprocessed.csv\"\n",
    "    log_t0 =  work_dir + \"log/log_preprocess_t0.txt\"\n",
    "    \n",
    "    t1_gaf_file = work_dir + \"uniprot/raw_goa/t1/goa_uniprot_all.gaf.gz\"\n",
    "    t1_processed = work_dir + \"extracted_goa/t1_preprocessed.csv\"\n",
    "    log_t1 = work_dir + \"log/log_preprocess_t1.txt\"\n",
    "    \n",
    "    \n",
    "    cmd_preprocess_t0 = get_preprocess_cmd(t0_gaf_file, t0_processed)\n",
    "    cmd_preprocess_t1 = get_preprocess_cmd(t1_gaf_file, t1_processed)\n",
    "    \n",
    "    # Preprocess both files\n",
    "    #run_process(cmd_preprocess_t0)\n",
    "    #run_process(cmd_preprocess_t1)\n",
    "    \n",
    "    \n",
    "    # Map the IDs of the processed \n",
    "    # TODO - To map or not to be mapped should be decided by a switch -m that the user can pass\n",
    "    mapping_file = \"/data/rashika/CAFA4/CAFA4_gt/Target_Entry_map.csv\"\n",
    "    t0_mapped = work_dir + \"mapped/t0_mapped.csv\"\n",
    "    t1_mapped = work_dir + \"mapped/t1_mapped.csv\"\n",
    "    map_goa_to_cafa_ids(t0_processed, mapping_file, 0, t0_mapped, chunk_size=100000)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61800d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:37<00:00, 48.60s/it]\n",
      " 50%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                   | 1/2 [00:00<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(goa_path='/data/rashika/CAFA4/uniprot/raw_goa/sample_t0.gz', extract_col_list=['DB Object ID', 'Qualifier', 'GO ID', 'Evidence Code', 'Aspect'], no_dup=True, no_neg=True, evidence_codes=['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC'], highTP=True, out_path='/data/rashika/CAFA4/extracted_goa/t1_sample.csv', only_annot=True)\n",
      "Indices of the extracted columns are :  [1, 3, 4, 6, 8]\n",
      "9992  Rows in the input file\n",
      "2832  duplicates dropped\n",
      "0  Not Qualifiers found\n",
      "High Thoughput Evidence Code included\n",
      "Included evidence codes are : ['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC', 'HTP', 'HDA', 'HMP', 'HGI', 'HEP']\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [DB Object ID, Qualifier, GO ID, Evidence Code, Aspect]\n",
      "Index: []\n",
      "Index(['DB Object ID', 'Qualifier', 'GO ID', 'Evidence Code', 'Aspect'], dtype='object')\n",
      "Namespace(goa_path='/data/rashika/CAFA4/uniprot/raw_goa/sample_t0.gz', extract_col_list=['DB Object ID', 'Qualifier', 'GO ID', 'Evidence Code', 'Aspect'], no_dup=True, no_neg=True, evidence_codes=['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC'], highTP=True, out_path='/data/rashika/CAFA4/extracted_goa/t0_sample.csv', only_annot=True)\n",
      "Indices of the extracted columns are :  [1, 3, 4, 6, 8]\n",
      "9992  Rows in the input file\n",
      "2832  duplicates dropped\n",
      "0  Not Qualifiers found\n",
      "High Thoughput Evidence Code included\n",
      "Included evidence codes are : ['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC', 'HTP', 'HDA', 'HMP', 'HGI', 'HEP']\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [DB Object ID, Qualifier, GO ID, Evidence Code, Aspect]\n",
      "Index: []\n",
      "Index(['DB Object ID', 'Qualifier', 'GO ID', 'Evidence Code', 'Aspect'], dtype='object')\n",
      "All work done\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "NUMBER_OF_TASKS = 2\n",
    "progress_bar = tqdm(total=NUMBER_OF_TASKS)\n",
    "\n",
    "\n",
    "def work(gaf_path, out_path):\n",
    "    cmd = [\n",
    "    \"python3\",                 # Command to execute Python 3\n",
    "    \"preprocess_gaf.py\",       # Script to run\n",
    "    gaf_file,  # Path to input file\n",
    "    \"--highTP\",\n",
    "    \"--out_path\", out_path,        # Output path parameter\n",
    "    #\"--evidence_codes\", \"EXP\", \"IDA\",   # Evidence codes parameter\n",
    "    #\"--extract_col_list\", \"DB Object ID\", \"Qualifier\"  # Extract column list parameter\n",
    "]\n",
    "    subprocess.call(cmd)\n",
    "\n",
    "\n",
    "def update_progress_bar(_):\n",
    "    progress_bar.update()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(NUMBER_OF_TASKS)\n",
    "    \n",
    "    # Define commands and log file names\n",
    "    work_dir = \"/data/rashika/CAFA4/\"\n",
    "    \n",
    "    t0_gaf_file = work_dir + \"uniprot/raw_goa/sample_t0.gz\"\n",
    "    #t0_gaf_file = work_dir + \"uniprot/raw_goa/t0/goa_uniprot_all.gaf.195.gz\"\n",
    "    #t0_out_path = work_dir + \"extracted_goa/t0_preprocessed.csv\"\n",
    "    t0_out_path = work_dir + \"extracted_goa/t0_sample.csv\"\n",
    "    #log_t0 =  work_dir + \"log/log_preprocess_t0.txt\"\n",
    "    log_t0 =  work_dir + \"log/log_preprocess_t0_sample.txt\"\n",
    "    \n",
    "    t1_gaf_file = work_dir + \"uniprot/raw_goa/sample_t1.gz\"\n",
    "    #t1_gaf_file = work_dir + \"uniprot/raw_goa/t1/goa_uniprot_all.gaf.gz\"\n",
    "    #t1_out_path = work_dir + \"extracted_goa/t1_preprocessed.csv\"\n",
    "    t1_out_path = work_dir + \"extracted_goa/t1_sample.csv\"\n",
    "    #log_t1 = work_dir + \"log/log_preprocess_t1.txt\"\n",
    "    log_t1 =  work_dir + \"log/log_preprocess_t1_sample.txt\"\n",
    "    \n",
    "    pool.apply_async(work, (t0_gaf_file, t0_out_path), callback=update_progress_bar)\n",
    "    pool.apply_async(work, (t1_gaf_file, t1_out_path), callback=update_progress_bar)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"All work done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abdca21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [18:37<00:00, 279.50s/it]\n",
      " 25%|█████████████████████████████████████████▊                                                                                                                             | 1/4 [00:01<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just did some hard work for 1s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████████████████████████████████████████████████▌                                                                                   | 2/4 [00:02<00:02,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just did some hard work for 2s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 3/4 [00:03<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just did some hard work for 3s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just did some hard work for 4s!\n",
      "All work done\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "NUMBER_OF_TASKS = 4\n",
    "progress_bar = tqdm(total=NUMBER_OF_TASKS)\n",
    "\n",
    "\n",
    "def work(sec_sleep):\n",
    "    command = ['python', 'worker.py', sec_sleep]\n",
    "    subprocess.call(command)\n",
    "\n",
    "\n",
    "def update_progress_bar(_):\n",
    "    progress_bar.update()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = mp.Pool(NUMBER_OF_TASKS)\n",
    "\n",
    "    for seconds in [str(x) for x in range(1, NUMBER_OF_TASKS + 1)]:\n",
    "        pool.apply_async(work, (seconds,), callback=update_progress_bar)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print(\"All work done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a3dd7",
   "metadata": {},
   "source": [
    "extract_annot()\n",
    "Extracts the GO annotations from raw .gaf files. \n",
    "Removes duplicated, negative annotations, extract the annotations only with the given evidence codes\t\n",
    "\n",
    "- gaf_file: path of the .gaf file\n",
    "- evidence codes = [EXP, IPI, IDA, IMP, IGI, IEP, TAS, IC, HTP, HDA, HMP, HGI, HEP]: optional.\n",
    "Default: experimental + high throughput codes \n",
    "- ann_file: Path + name of the extrracted annotations file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "14520788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(goa_path='/data/rashika/CAFA4/uniprot/caution/sample_t0.gz', extract_col_list=['DB Object ID', 'Qualifier', 'GO ID', 'Evidence Code', 'Aspect'], no_dup=True, no_neg=True, evidence_codes=['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC'], highTP=True, out_path='extracted.tsv', only_annot=True)\n",
      "Indices of the extracted columns are :  [1, 3, 4, 6, 8]\n",
      "9992  Rows in the input file\n",
      "2832  duplicates dropped\n",
      "0  Not Qualifiers found\n",
      "High Thoughput Evidence Code included\n",
      "Included evidence codes are : ['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC', 'HTP', 'HDA', 'HMP', 'HGI', 'HEP']\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [DB Object ID, Qualifier, GO ID, Evidence Code, Aspect]\n",
      "Index: []\n",
      "Index(['DB Object ID', 'Qualifier', 'GO ID', 'Evidence Code', 'Aspect'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python3', 'preprocess_gaf.py', '/data/rashika/CAFA4/uniprot/caution/sample_t0.gz', '--highTP'], returncode=0)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = [\n",
    "    \"python3\",                 # Command to execute Python 3\n",
    "    \"preprocess_gaf.py\",       # Script to run\n",
    "    \"/data/rashika/CAFA4/uniprot/caution/sample_t0.gz\",  # Path to input file\n",
    "    \"--highTP\",\n",
    "    #\"--out_path\", \"bdjashdajks\",        # Output path parameter\n",
    "    #\"--evidence_codes\", \"EXP\", \"IDA\",   # Evidence codes parameter\n",
    "    #\"--extract_col_list\", \"DB Object ID\", \"Qualifier\"  # Extract column list parameter\n",
    "]\n",
    "\n",
    "# Call the script with subprocess\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "79d7b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_sample = \"/data/rashika/CAFA4/uniprot/caution/sample_t0.gz\"\n",
    "t1_sample = \"/data/rashika/CAFA4/uniprot/caution/sample_t1.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "dd2ed0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of the extracted columns are :  [1, 3, 4, 6, 8]\n",
      "Indices of the extracted columns are :  [1, 3, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "t0_df = extract_annot(t0_sample)\n",
    "t1_df = extract_annot(t1_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "afd5ee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9992  Rows\n",
      "2832  duplicates dropped\n",
      "0  Not Qualifiers found\n",
      "High Thoughput Evidence Code included\n",
      "Included evidence codes are : ['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC', 'HTP', 'HDA', 'HMP', 'HGI', 'HEP', 'HTP', 'HDA', 'HMP', 'HGI', 'HEP']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "Extracted_ann = remove_dup_and_neg(t0_df)\n",
    "f = filter_evidence_codes(Extracted_ann, highTP = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0b2a00ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['IEA'], dtype=object)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0_df[\"Evidence Code\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "48521b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       False\n",
      "1       False\n",
      "2       False\n",
      "3       False\n",
      "4       False\n",
      "        ...  \n",
      "9984    False\n",
      "9985    False\n",
      "9986    False\n",
      "9987    False\n",
      "9990    False\n",
      "Name: Qualifier, Length: 7165, dtype: bool\n",
      "7165\n",
      "7165\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annot(goa_file, out_file=\"extracted.csv\", gaf_version = \"2.2\"):\n",
    "    \"\"\"\n",
    "    Extracts columns from a GOA file and writes them to a new file.\n",
    "\n",
    "    Parameters:\n",
    "    - goa_file: The input GOA file name.\n",
    "    - n_skip: No. of rows to be skipped. Default value 9.\n",
    "    - out_file: (Optional) The output file name. Defaults to 'extracted.csv'.\n",
    "    - col_list: List of column IDs to be extracted, e.g., [1, 5]. Defaults to [1, 4].\n",
    "    \"\"\"\n",
    "    with gzip.open(goa_file, 'rt') as f:\n",
    "        # Skip the first 8 lines\n",
    "        for _ in range(n_skip):\n",
    "            next(f)\n",
    "\n",
    "        # Create a CSV reader object with tab delimiter\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "\n",
    "        # Open the output file for writing\n",
    "        with open(out_file, 'w') as outfile:\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.writer(outfile, delimiter='\\t')\n",
    "            \n",
    "            # Iterate over each row in the reader\n",
    "            for row in reader:\n",
    "                # Extract the specified columns\n",
    "                extracted_columns = [row[i] for i in col_list]\n",
    "\n",
    "                # Write the extracted columns to the output file\n",
    "                writer.writerow(extracted_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab76a9",
   "metadata": {},
   "source": [
    "### Extract Entry ID, GO annotation, and ontology type from the gaf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d894b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths\n",
    "t0_out_dir = '/data/rashika/CAFA4/uniprot/goa_2020_Jan_03/'\n",
    "t0_input_file = t0_out_dir + 'goa_uniprot_all.gaf.gz'\n",
    "t0_output_file = t0_out_dir + 'extracted_columns.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_col_list = [1,4, 8]\n",
    "n_skip = 8\n",
    "\n",
    "#extract_annot(t0_input_file, n_skip, t0_output_file, t0_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f1809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths\n",
    "t1_out_dir = '/data/rashika/CAFA4/uniprot/goa_2024-02-09/'\n",
    "t1_input_file = t1_out_dir + 'goa_uniprot_all.gaf.gz'\n",
    "t1_output_file = t1_out_dir + 'extracted_columns.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_col_list  = [1,3,4,6,8]\n",
    "n_skip = 9\n",
    "#extract_annot(t1_input_file, n_skip, t1_output_file, t1_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract annotations from the file used by Shawn\n",
    "shawn_t0_dir = '/data/yisupeng/sharing/cafa4/'\n",
    "in_file = shawn_t0_dir + 'goa_uniprot_all_02142020.gaf.gz'\n",
    "shawn_out_file = '/data/rashika/CAFA4/uniprot/'+ 'shawn_extracted_columns.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8963901",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list  = [1,4, 6, 8]\n",
    "n_skip = 8\n",
    "#extract_annot(in_file, n_skip, shawn_out_file, col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a1527",
   "metadata": {},
   "source": [
    "### Map the Extracted annotations to the CAFA targets (by Entry ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88deb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read a CSV file in chunks, map the primary ID to a mapping file, and keep the rows that can be mapped.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the CSV file.\n",
    "    - mapping_file: Path to the mapping file (CSV).\n",
    "    - primary_id_column: Name of the column containing the primary ID in the mapping file.\n",
    "    - out_path: Path to the output file.\n",
    "    - chunk_size: Size of each chunk. Defaults to 100,000 lines.\n",
    "    \"\"\"\n",
    "def map_goa_to_cafa_ids(file_path, mapping_file, primary_id_column, out_path, chunk_size=100000):\n",
    "    # Read the mapping file into a DataFrame\n",
    "    mapping_df = pd.read_csv(mapping_file, sep = \",\", header = 0)\n",
    "    mapping_df.columns = [\"Entry\", \"CAFA4_ID\"]\n",
    "\n",
    "    # Extract the primary IDs from the mapping file and convert to a set for efficient lookup\n",
    "    id_set = set(mapping_df[\"Entry\"])\n",
    "\n",
    "    # Initialize an empty list to store filtered chunk dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Read the CSV file in chunks\n",
    "    #flag = 0\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size, sep = \"\\t\"):\n",
    "        # Filter the chunk based on whether the primary ID can be found in the mapping file\n",
    "        filtered_chunk = chunk[chunk.iloc[:,primary_id_column].isin(id_set)]\n",
    "        filtered_chunk = filtered_chunk.drop_duplicates().copy()\n",
    "        dfs.append(filtered_chunk)\n",
    "\n",
    "    # Concatenate all the filtered chunk dataframes into a single dataframe\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Write the final dataframe to the output file\n",
    "    df.to_csv(out_path, index=False, sep = \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "60228f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapping_file = \"/data/rashika/CAFA4/CAFA4_gt/Target_Entry_map.csv\"\n",
    "\n",
    "#Mapping_df = pd.read_csv(Mapping_file,  sep = ',', header = None)\n",
    "#Mapping_df.columns = [\"Entry\", \"CAFA4_ID\"]\n",
    "\n",
    "t1_mapped_ann = \"/data/rashika/CAFA4/CAFA4_gt/t1_ann.csv\"\n",
    "t0_mapped_ann = \"/data/rashika/CAFA4/CAFA4_gt/t0_ann.csv\"\n",
    "shawn_t0_mapped_ann = \"/data/rashika/CAFA4/CAFA4_gt/shawn_t0_ann.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db763c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clara_Entry_IDs = \"/data/rashika/CAFA4/CAFA4_gt/Entry.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Clara_Entry_IDs = pd.read_csv(Clara_Entry_IDs,  sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mapping_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map t1 annotations\n",
    "#map_goa_to_cafa_ids(t1_output_file, Mapping_file, 0, t1_mapped_ann )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf454e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map t0 annotations\n",
    "#map_goa_to_cafa_ids(t0_output_file, Mapping_file, 0, t0_mapped_ann )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef03582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Shawn's annotations\n",
    "#map_goa_to_cafa_ids(shawn_out_file, Mapping_file, 0, shawn_t0_mapped_ann )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "57db1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://geneontology.org/docs/guide-go-evidence-codes/\n",
    "#Exp_codes = ['EXP', 'IDA', 'IMP', 'IGI', 'IEP', 'TAS', 'IC' ]\n",
    "Evidence_codes = ['EXP', 'IDA', 'IPI','IMP', 'IGI', 'IEP', 'TAS', 'IC', 'HTP', 'HDA', 'HMP', 'HGI', 'HEP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c7673828",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(t1_mapped_ann,  sep = '\\t', header = None)\n",
    "t1.columns = ['Entry', 'edge', 'term', \"E_code\", \"aspect\"]\n",
    "\n",
    "\n",
    "# TO do\n",
    "\n",
    "# Write function to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c2a11ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EXP', 'HDA', 'HEP', 'HGI', 'HMP', 'HTP', 'IBA', 'IC', 'IDA',\n",
       "       'IEA', 'IEP', 'IGC', 'IGI', 'IKR', 'IMP', 'IPI', 'ISA', 'ISM',\n",
       "       'ISO', 'ISS', 'NAS', 'ND', 'RCA', 'TAS'], dtype=object)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(t1.loc[:,\"E_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "78d6a0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NOT|acts_upstream_of', 'NOT|acts_upstream_of_or_within',\n",
       "       'NOT|acts_upstream_of_or_within_negative_effect',\n",
       "       'NOT|acts_upstream_of_or_within_positive_effect',\n",
       "       'NOT|colocalizes_with', 'NOT|contributes_to', 'NOT|enables',\n",
       "       'NOT|involved_in', 'NOT|is_active_in', 'NOT|located_in',\n",
       "       'NOT|part_of', 'acts_upstream_of',\n",
       "       'acts_upstream_of_negative_effect', 'acts_upstream_of_or_within',\n",
       "       'acts_upstream_of_or_within_negative_effect',\n",
       "       'acts_upstream_of_or_within_positive_effect',\n",
       "       'acts_upstream_of_positive_effect', 'colocalizes_with',\n",
       "       'contributes_to', 'enables', 'involved_in', 'is_active_in',\n",
       "       'located_in', 'part_of'], dtype=object)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(t1.edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a9b7b55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2711301061495732"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t1.loc[:,\"E_code\"].isin(Evidence_codes))/len(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc04de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = t1[t1.loc[:,\"E_code\"].isin(Evidence_codes)].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e98e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "shawn_t0 = pd.read_csv(shawn_t0_mapped_ann,  sep = '\\t', header = None)\n",
    "shawn_t0.columns = ['Entry', 'term', 'E_code','aspect']\n",
    "shawn_t0 = shawn_t0[shawn_t0.loc[:,\"E_code\"].isin(Evidence_codes)].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(shawn_t0['Entry']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59937da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(t1['Entry']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce24373",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_mapped = pd.merge(t1, Mapping_df, on='Entry', how='inner')\n",
    "t1_mapped\n",
    "t1_mapped = t1_mapped.loc[:, [\"CAFA4_ID\", \"term\", \"aspect\", \"edge\"]]\n",
    "t1_mapped.to_csv('/data/rashika/CAFA4/CAFA4_gt/t1_mapped.csv', sep = \"\\t\", index=False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66638325",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_mapped = pd.merge(shawn_t0, Mapping_df, on='Entry', how='inner')\n",
    "t0_mapped\n",
    "t0_mapped = t0_mapped.loc[:, [\"CAFA4_ID\", \"term\", \"aspect\"]]\n",
    "t0_mapped.to_csv('/data/rashika/CAFA4/CAFA4_gt/t0_mapped.csv', sep = \"\\t\",index=False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4592920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
