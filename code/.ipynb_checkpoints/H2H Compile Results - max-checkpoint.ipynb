{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056432b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from make_benchmarks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f9ec2",
   "metadata": {},
   "source": [
    "Combine cafa3_1 and cafa3_2 results, and cafa2_1 and cafa2_1 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = '/data/rashika/CAFA4/head2head/collected_eval/'\n",
    "\n",
    "# Cafa2 paths\n",
    "cafa2_1_path = eval_path + 'eval_cafa2_1/'\n",
    "cafa2_2_path = eval_path + 'eval_cafa2_2/'\n",
    "eval_cafa2 = eval_path + \"eval_cafa2/\"\n",
    "\n",
    "# Cafa3 paths\n",
    "cafa3_1_path = eval_path + 'eval_cafa3_1/'\n",
    "cafa3_2_path = eval_path + 'eval_cafa3_2/'\n",
    "eval_cafa3 = eval_path + \"eval_cafa3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the results in the results_paths_list and outputs them in combined_results_path\n",
    "def combine_results(results_paths_list, combined_results_path, add_result_path = False):\n",
    "    \n",
    "    #Collect the benchmark folders in each result path\n",
    "    benchmarks = []\n",
    "    for result_path in results_paths_list:\n",
    "        benchmarks.append(set(os.listdir(result_path)))\n",
    "        \n",
    "    #Get the common benchmark folders\n",
    "    benchmarks = list(set.intersection(*benchmarks))\n",
    "    print(\"Common benchmarks = \", benchmarks)\n",
    "    \n",
    "    # Combine the results in the common benchmarks and save then at the combined_results_path\n",
    "    for bm in benchmarks:\n",
    "        if not os.path.exists(combined_results_path + bm):\n",
    "            os.mkdir(combined_results_path + bm)\n",
    "            \n",
    "        result_files = []\n",
    "        for result_path in results_paths_list:\n",
    "            result_files.append(set(os.listdir(result_path + bm)))\n",
    "        \n",
    "        \n",
    "        #Get the common result files\n",
    "        result_files = list(set.intersection(*result_files))\n",
    "        print(\"Common Result files = \", result_files)\n",
    "        \n",
    "        \n",
    "        for result_file in result_files:\n",
    "            dfs = []\n",
    "            for result_path in results_paths_list:\n",
    "                df = pd.read_csv(result_path + bm + \"/\"+result_file, sep = '\\t')\n",
    "                if add_result_path:\n",
    "                    print(result_path.split(\"/\"))\n",
    "                    df[\"Result Path\"] = result_path.split(\"/\")[-2]\n",
    "                    df[\"cafa\"] = result_path.split(\"/\")[-2].split(\"_\")[1]\n",
    "                    df[\"cafa_filename\"] = df[\"cafa\"] + \"_\" + df[\"filename\"]\n",
    "                dfs.append(df)\n",
    "            \n",
    "            df = pd.concat(dfs, axis = 0)\n",
    "            display(df)\n",
    "            df.to_csv(combined_results_path + bm + \"/\" + result_file, sep = \"\\t\", index = None, header = True)\n",
    "        \n",
    "        #mets_plot = sns.displot([df1.f, df.f], kind=\"kde\",common_norm=False)\n",
    "        #mets_plot.set(yticks=[])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe23f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CAFA2_1 and CAFA2_2 results\n",
    "#combine_results([cafa2_1_path, cafa2_2_path], eval_cafa2, add_result_path = False)\n",
    "\n",
    "# Combine CAFA3_1 and CAFA3_2 results\n",
    "#combine_results([cafa3_1_path, cafa3_2_path], eval_cafa3, add_result_path = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de2273",
   "metadata": {},
   "source": [
    "Combine CAFA2, CAFA3, CAFA4 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443359bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cafa4 = eval_path + \"eval_cafa4/\"\n",
    "combined_results = eval_path + \"combined_results/\"\n",
    "combine_results([eval_cafa2, eval_cafa3, eval_cafa4], combined_results, add_result_path = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673d1cd",
   "metadata": {},
   "source": [
    "### Distribution of f_max_micro and S_min for each CAFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually observe the y_limits for S_min distributions\n",
    "S_min_y_lim = {}\n",
    "S_min_y_lim['bpo_all_type1'] = [15, 40]\n",
    "S_min_y_lim['bpo_all_type2'] = [15, 40]\n",
    "S_min_y_lim['bpo_all_type3'] = [4, 120]\n",
    "S_min_y_lim['bpo_all_type12'] = [15, 40]\n",
    "S_min_y_lim['cco_all_type1'] = [3, 15]\n",
    "S_min_y_lim['cco_all_type2'] = [4, 12]\n",
    "S_min_y_lim['cco_all_type3'] = [4, 25]\n",
    "S_min_y_lim['cco_all_type12'] = [5, 12]\n",
    "S_min_y_lim['mfo_all_type1'] = [0, 12]\n",
    "S_min_y_lim['mfo_all_type2'] = [0, 15]\n",
    "S_min_y_lim['mfo_all_type3'] = [0, 25]\n",
    "S_min_y_lim['mfo_all_type12'] = [0, 15]\n",
    "\n",
    "benchmarks = os.listdir(combined_results)\n",
    "for bm in benchmarks:\n",
    "    f_micro_df = pd.read_csv(combined_results + bm + \"/evaluation_best_f_micro.tsv\", sep = \"\\t\", header = 0)\n",
    "    \n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 2.5))  \n",
    "\n",
    "    #plt.figure()\n",
    "    plot = sns.boxplot(x='Result Path', y='f', data = f_micro_df.loc[:, [\"f\", \"Result Path\"]], ax=axes[0])\n",
    "    axes[0].set_title(bm)\n",
    "    axes[0].set_xlabel(\"f_max\")\n",
    "    \n",
    "    S_min_df = pd.read_csv(combined_results + bm + \"/evaluation_best_s.tsv\", sep = \"\\t\", header = 0)\n",
    "    plot = sns.boxplot(x='Result Path', y='s', data = S_min_df.loc[:, [\"s\", \"Result Path\"]], ax=axes[1])\n",
    "    axes[1].set_xlabel(\"S_min\")\n",
    "    axes[1].set_ylim(S_min_y_lim[bm])\n",
    "    axes[1].set_title(bm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da61d662",
   "metadata": {},
   "source": [
    "Combine the register files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cafa2_register = eval_path + \"cafa2_register.tab\"\n",
    "cafa3_register = eval_path + \"cafa3_register.tab\"\n",
    "cafa4_register = eval_path + \"cafa4_register.tab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e619bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "registers = [cafa2_register, cafa3_register, cafa4_register]\n",
    "dfs = []\n",
    "for register in registers:\n",
    "    df = pd.read_csv(register, sep = \"\\t\", header = 0)\n",
    "    df.columns = ['#INTERNAL_ID', 'EXTERNAL_ID', 'TEAM_NAME', 'TYPE', 'DISPLAY_NAME',\n",
    "       'DUMP_NAME', 'PI', 'KEYWORDS', 'COLOR (RGB in HEX)']\n",
    "    df[\"CAFA\"] = register.split(\"/\")[-1].split(\"_\")[0]\n",
    "    dfs.append(df)\n",
    "    \n",
    "combined_register = pd.concat(dfs, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_register.to_csv(combined_results + \"Combined_register.tab\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ee261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots_h2_h(results_path, metric, cols,out_path='/home/rashika/CAFA4/eval/plots/', n_curves = None, names_file = None, S_min_coord = None):\n",
    "    dir_list = os.listdir(results_path)\n",
    "    \n",
    "    cumulate = True\n",
    "    add_extreme_points = True\n",
    "    coverage_threshold = 0.3\n",
    "    axis_title_dict = {'pr': 'Precision', 'rc': 'Recall', 'f': 'F-score', 'pr_w': 'Weighted Precision', 'rc_w': 'Weighted Recall', 'f_w': 'Weighted F-score', 'mi': 'Misinformation (Unweighted)', 'ru': 'Remaining Uncertainty (Unweighted)', 'mi_w': 'Misinformation', 'ru_w': 'Remaining Uncertainty', 's': 'S-score', 'pr_micro': 'Precision (Micro)', 'rc_micro': 'Recall (Micro)', 'f_micro': 'F-score (Micro)', 'pr_micro_w': 'Weighted Precision (Micro)', 'rc_micro_w': 'Weighted Recall (Micro)', 'f_micro_w': 'Weighted F-score (Micro)'}\n",
    "    ontology_dict = {'biological_process': 'BPO', 'molecular_function': 'MFO', 'cellular_component': 'CCO'}\n",
    "    \n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "    \n",
    "    for file in dir_list:\n",
    "        df_file = results_path + file +\"/evaluation_all.tsv\"\n",
    "        df = pd.read_csv(df_file, sep=\"\\t\")\n",
    "        out_folder = out_path + file\n",
    "        if not os.path.exists(out_folder):\n",
    "            os.mkdir(out_folder)\n",
    "            \n",
    "        \n",
    "        df = pd.read_csv(df_file, sep=\"\\t\")\n",
    "        \n",
    "        # Set method information (optional)\n",
    "        if names_file is None:\n",
    "            df['group'] = df['cafa_filename']\n",
    "            df['label'] = df['cafa_filename']\n",
    "            df['is_baseline'] = False\n",
    "        else:\n",
    "            methods = pd.read_csv(names_file, sep = \"\\t\", header=0)\n",
    "            df = pd.merge(df, methods, on='cafa_filename', how='left')\n",
    "            df['group'].fillna(df['cafa_filename'], inplace=True)\n",
    "            df['label'].fillna(df['cafa_filename'], inplace=True)\n",
    "            if 'is_baseline' not in df:\n",
    "                df['is_baseline'] = False\n",
    "            else:\n",
    "                df['is_baseline'].fillna(False, inplace=True)\n",
    "            # print(methods)\n",
    "        #df = df.drop(columns='cafa_filename').set_index(['group', 'label', 'ns', 'tau'])\n",
    "        df = df.set_index(['group_unique', 'label', 'ns', 'cafa_filename','tau'])\n",
    "        \n",
    "        # Filter by coverage\n",
    "        df = df[df['cov'] >= coverage_threshold]\n",
    "        \n",
    "        # Assign colors based on group\n",
    "        if 'colors' not in df.columns:\n",
    "            cmap = plt.get_cmap('tab20')\n",
    "            df['colors'] = df.index.get_level_values('group_unique')\n",
    "            df['colors'] = pd.factorize(df['colors'])[0]\n",
    "            df['colors'] = df['colors'].apply(lambda x: cmap.colors[x % len(cmap.colors)])\n",
    "\n",
    "        index_best = df.groupby(level=['group_unique', 'ns'])[metric].idxmax() if metric in ['f', 'f_w', 'f_micro', 'f_micro_w'] else df.groupby(['group_unique', 'ns'])[metric].idxmin()\n",
    "        \n",
    "        # Filter the dataframe for the best methods\n",
    "        df_methods = df.reset_index('tau').loc[[ele[:-1] for ele in index_best], ['tau', 'cov', 'colors'] + cols + [metric]].sort_index()\n",
    "\n",
    "        # Makes the curves monotonic. Cumulative max on the last column of the cols variable, e.g. \"pr\" --> precision\n",
    "        if cumulate:\n",
    "            if metric in ['f', 'f_w', 'f_micro', 'f_micro_w']:\n",
    "                df_methods[cols[-1]] = df_methods.groupby(level=['label', 'ns'])[cols[-1]].cummax()\n",
    "            else:\n",
    "                df_methods[cols[-1]] = df_methods.groupby(level=['label', 'ns'])[cols[-1]].cummin()\n",
    "\n",
    "\n",
    "        # Save to file\n",
    "        df_methods.drop(columns=['colors']).to_csv('{}/fig_{}.tsv'.format(out_folder, metric), float_format=\"%.3f\", sep=\"\\t\")\n",
    "        \n",
    "        # Add first last points to precision and recall curves to improve APS calculation\n",
    "        #def add_points(df_):\n",
    "        #    df_ = pd.concat([df_.iloc[0:1], df_])\n",
    "        #    df_.iloc[0, df_.columns.get_indexer(['tau', cols[0], cols[1]])] = [0, 1, 0]  # tau, rc, pr\n",
    "        #    df_ = pd.concat([df_, df_.iloc[-1:]])\n",
    "        #    df_.iloc[-1, df_.columns.get_indexer(['tau', cols[0], cols[1]])] = [1.1, 0, 1]\n",
    "        #    return df_\n",
    "\n",
    "        #if metric.startswith('f') and add_extreme_points:\n",
    "        #    df_methods = df_methods.reset_index().groupby(['group_unique', 'label', 'ns'], as_index=False).apply(add_points).set_index(['group_unique', 'label', 'ns'])\n",
    "        \n",
    "        # Filter the dataframe for the best method and threshold\n",
    "        df_best = df.loc[index_best, ['cov', 'colors'] + cols + [metric]]\n",
    "        \n",
    "        # Calculate average precision score \n",
    "        #if metric.startswith('f'):\n",
    "        #    df_best['aps'] = df_methods.groupby(level=['group_unique', 'label', 'ns'])[[cols[0], cols[1]]].apply(lambda x: (x[cols[0]].diff(-1).shift(1) * x[cols[1]]).sum())\n",
    "\n",
    "        # Calculate the max coverage across all thresholds\n",
    "        df_best['max_cov'] = df_methods.groupby(level=['group_unique', 'label', 'ns'])['cov'].max()\n",
    "        \n",
    "        # Set a label column for the plot legend\n",
    "        df_best['label'] = df_best.index.get_level_values('group_unique')\n",
    "        df_best['label'] = df_best.agg(lambda x: f\"{x['label']} ({metric.upper()}={x[metric]:.3f} C={x['max_cov']:.3f})\", axis=1)\n",
    "#         if 'aps' not in df_best.columns:\n",
    "#             df_best['label'] = df_best.agg(lambda x: f\"{x['label']} ({metric.upper()}={x[metric]:.3f} C={x['max_cov']:.3f})\", axis=1)\n",
    "#         else:\n",
    "#             df_best['label'] = df_best.agg(lambda x: f\"{x['label']} ({metric.upper()}={x[metric]:.3f} APS={x['aps']:.3f} C={x['max_cov']:.3f})\", axis=1)\n",
    "        \n",
    "        # Generate the figures\n",
    "        plt.rcParams.update({'font.size': 22, 'legend.fontsize': 18})\n",
    "\n",
    "        # F-score contour lines\n",
    "        x = np.arange(0.01, 1, 0.01)\n",
    "        y = np.arange(0.01, 1, 0.01)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = 2 * X * Y / (X + Y)\n",
    "\n",
    "        \n",
    "        for ns, df_g in df_best.groupby(level='ns'):\n",
    "            fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "             # Contour lines. At the moment they are provided only for the F-score\n",
    "            if metric.startswith('f'):\n",
    "                CS = ax.contour(X, Y, Z, np.arange(0.1, 1.0, 0.1), colors='gray')\n",
    "                ax.clabel(CS, inline=True) #, fontsize=10)\n",
    "\n",
    "            cnt = -1\n",
    "            # Iterate methods\n",
    "            for i, (index, row) in enumerate(df_g.sort_values(by=[metric, 'max_cov'], ascending=[False if metric.startswith('f') else True, False]).iterrows()):\n",
    "                \n",
    "                cnt+=1\n",
    "                #print(row)\n",
    "                if (n_curves and cnt <= n_curves) or ('BLAST' in row['label']) or ('Naive' in row['label']):\n",
    "                \n",
    "                    #data = df_methods.loc[index[:-1]]\n",
    "\n",
    "                    data = df_methods.loc[index[:-2]]\n",
    "                    print(row[cols[0]], row[cols[1]])\n",
    "    \n",
    "    \n",
    "                    # Precision-recall or mi-ru curves\n",
    "                    if ('BLAST' in row['label']) or ('Naive' in row['label']):\n",
    "                        ax.plot(data[cols[0]], data[cols[1]], linestyle='dotted', color=row['colors'], label=row['label'], lw=3.5, zorder=500-i)\n",
    "                    else:\n",
    "                        ax.plot(data[cols[0]], data[cols[1]], color=row['colors'], label=row['label'], lw=3.5, zorder=500-i)\n",
    "\n",
    "                    # F-max or S-min dots\n",
    "                    ax.plot(row[cols[0]], row[cols[1]], color=row['colors'], marker='o', markersize=12, mfc='none', zorder=1000-i)\n",
    "                    ax.plot(row[cols[0]], row[cols[1]], color=row['colors'], marker='o', markersize=6, zorder=1000-i)\n",
    "\n",
    "                \n",
    "                \n",
    "            # Set axes limit\n",
    "            print(file)\n",
    "            if metric.startswith('f') and S_min_coord:\n",
    "                plt.xlim(S_min_coord[file][0])\n",
    "                plt.ylim(S_min_coord[file][1])\n",
    "            \n",
    "            #Set axes limit\n",
    "            #if metric.startswith('s'):\n",
    "            #    plt.xlim(23, 28)\n",
    "            #    plt.ylim(0, 50)\n",
    "                #plt.xlim(0.4*df_best.loc[:,:,ns,:][cols[0]].max(), df_best.loc[:,:,ns,:][cols[0]].max())\n",
    "                #plt.ylim(0, 1)\n",
    "\n",
    "            # plt.xlim(0, max(1, df_best.loc[:,:,ns,:][cols[0]].max()))\n",
    "            # plt.ylim(0, max(1, df_best.loc[:,:,ns,:][cols[1]].max()))\n",
    "\n",
    "            # Set titles\n",
    "            ax.set_title(file)\n",
    "            ax.set_xlabel(axis_title_dict[cols[0]], labelpad=20, fontsize=36)\n",
    "            ax.set_ylabel(axis_title_dict[cols[1]], labelpad=20, fontsize=36)\n",
    "\n",
    "            # Legend\n",
    "            #ax.legend(loc='center right', bbox_to_anchor=(1, 1.5))\n",
    "            leg = ax.legend(markerscale=6, title=file)\n",
    "            for legobj in leg.get_lines():\n",
    "                legobj.set_linewidth(10.0)\n",
    "                \n",
    "            leg.set_bbox_to_anchor((1.65, 0.75))  \n",
    "\n",
    "            # Save figure on disk\n",
    "            plt.savefig(\"{}/fig_{}_{}.png\".format(out_folder, metric, ns), bbox_inches='tight', dpi=300, transparent=True)\n",
    "            # plt.clf()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_min_coord = {}\n",
    "S_min_coord['bpo_all_type1'] = [[17, 27], [0, 50]]\n",
    "S_min_coord['bpo_all_type2'] = [[12, 22], [0, 40]]\n",
    "S_min_coord['bpo_all_type3'] = [[16, 22], [0, 100]]\n",
    "S_min_coord['bpo_all_type12'] = [[15, 24], [0, 60]]\n",
    "S_min_coord['cco_all_type1'] = [[7, 10], [0, 20]]\n",
    "S_min_coord['cco_all_type2'] = [[5, 8], [0, 15]]\n",
    "S_min_coord['cco_all_type3'] = [[7, 10.5], [0, 15]]\n",
    "S_min_coord['cco_all_type12'] = [[6, 9], [0, 15]]\n",
    "S_min_coord['mfo_all_type1'] = [[2, 4.5], [0, 12]]\n",
    "S_min_coord['mfo_all_type2'] = [[3, 6], [0, 20]]\n",
    "S_min_coord['mfo_all_type3'] = [[7, 10], [0, 15]]\n",
    "S_min_coord['mfo_all_type12'] = [[3, 5], [0, 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = eval_path+ 'combined_plots/'\n",
    "plots_path_f = plots_path + 'f/'\n",
    "metric, cols = ('f', ['rc', 'pr'])\n",
    "register = '/data/rashika/CAFA4/head2head/collected_eval/register_combined_modified.tsv'\n",
    "create_plots_h2_h(combined_results, metric, cols, out_path = plots_path_f, n_curves = 10, names_file =register)\n",
    "\n",
    "plots_path_f_micro = plots_path + 'f_micro/'\n",
    "metric, cols =  ('f_micro', ['rc_micro', 'pr_micro'])\n",
    "create_plots_h2_h(combined_results, metric, cols, out_path = plots_path_f_micro, n_curves = 10, names_file =register)\n",
    "\n",
    "plots_path_s_w = plots_path + 's_w/'\n",
    "metric, cols = ('s_w', ['ru_w', 'mi_w'])\n",
    "create_plots_h2_h(combined_results, metric, cols, out_path = plots_path_s_w, n_curves = 10, names_file =register)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f55d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06137976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
